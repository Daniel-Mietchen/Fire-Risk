require 'sequel'

# Import from json "api" files because they contain parcel geometry,
# which the .csv files do not contain.

# parcel_boundary_area has more precision than parcel_area_details
# geometry field will be serialized json, or for postgres the json field type

# URLs
# - .json give metadata and aggregated data, including some stats measures
# - .geojson has the actual data including geometry
# - .csv has data but does NOT have geometry field
# TODO differences with sources
UUIDS =  {
  # with acreage, TODO need import for this
  real_estate_base_data:           'bc72d0590bf940ff952ab113f10a36a8_8',

  real_estate_residential_details: 'c7adfdab73104a01a485dec324adcafb_17',
  real_estate_commercial_details:  '17fbd0c459d84c71aa37b436d5231c0b_19',
  # real_estate_area:                '0e9946c2a77d4fc6ad16d9968509c588_72',
  master_address_table:            'dd9c7d93ed67438baefa3276c716f59d_5',

  # same url as real_estate_area
  parcel_area_details:             '0e9946c2a77d4fc6ad16d9968509c588_72',
  parcel_boundary_area:            '320d465ddf0d498796da6491e21f6dde_43',
}

# should really be part of the same table as UUIDS
# the uuids are part of these pages, but hard to scrape.
URLS =  {
  # with acreage, TODO need import for this
  real_estate_base_data:           'https://opendata.charlottesville.org/datasets/real-estate-base-data',

  real_estate_residential_details: 'https://opendata.charlottesville.org/datasets/real-estate-residential-details',
  real_estate_commercial_details:  'https://opendata.charlottesville.org/datasets/real-estate-commercial-details',
  # real_estate_area:                '0e9946c2a77d4fc6ad16d9968509c588_72',
  master_address_table:            'https://opendata.charlottesville.org/datasets/master-address-table',

  # same url as real_estate_area
  parcel_area_details:             'https://opendata.charlottesville.org/datasets/parcel-area-details',
  parcel_boundary_area:            'https://opendata.charlottesville.org/datasets/parcel-boundary-area',

  ########################
  # possibly not necessary
  real_estate_additions:           'https://opendata.charlottesville.org/datasets/real-estate-additions',

  # contains geospatial to address
  master_address_points:           'https://opendata.charlottesville.org/datasets/master-address-points',
}

URL_BASE = 'https://opendata.arcgis.com/datasets'
URL_BASE = 'https://opendata.arcgis.com/datasets/bc72d0590bf940ff952ab113f10a36a8_8.geojson'

def sources ext='geojson'
  UUIDS.keys.map{|uuid| "#{URL_BASE}/#{uuid}.#{ext}"}
end

# partly generated by field_types_of
# all tables join with ParcelNumber
def schema db
  db.create_table! :real_estate_base_data do
    Integer :RecordID_Int, primary_key: true
    String :ParcelNumber
    String :StreetNumber
    String :StreetName
    String :Unit
    String :StateCode
    String :TaxType
    String :Zone
    String :TaxDist
    String :Legal
    Integer :Acreage
    Integer :GPIN
  end

  db.create_table! :master_address_table do
    Integer :OBJECTID, primary_key: true
    String  :BIN
    String  :ST_NUMBER
    String  :ST_UNIT
    String  :UNIT_TYPE
    String  :PREDIR
    String  :ST_NAME
    String  :SUFFIX
    String  :POSTDIR
    String  :ZIP
    String  :STSEGID
    String  :USE_TYPE
    Integer :MasterAddressID
  end

  db.create_table! :parcel_area_details do
    Integer :OBJECTID # not primary key
    Integer :Assessment
    String  :FileType
    Integer :GeoParcelIdentificationNumber
    Integer :IsMultiParcelPolygon
    String  :Label
    String  :LegalDescription
    Float   :LotSquareFeet
    String  :MapPage
    String  :ModifiedDate
    String  :OwnerName
    String  :OwnerAddress
    String  :OwnerCityState
    String  :OwnerZipCode
    String  :ParcelNumber
    String  :StreetName
    String  :StreetNumber
    String  :TaxYear
    String  :Text
    String  :Unit
    String  :Zoning
    Integer :ESRI_OID
    String  :geometry
  end

  db.create_table! :real_estate_commercial_details do
    Integer :RecordID_Int, primary_key: true
    String  :ParcelNumber
    String  :UseCode
    Integer :YearBuilt
    String  :GrossArea
    Integer :StoryHeight
    String  :NumberOfStories
    String  :StreetName
    String  :StreetNumber
    String  :Unit
  end

  db.create_table! :real_estate_residential_details do
    Integer :RecordID_Int, primary_key: true
    String :ParcelNumber
    String :StreetNumber
    String :StreetName
    String :Unit
    String :UseCode
    String :Style
    String :Grade
    String :Roof
    String :Flooring
    String :Heating
    String :Fireplace
    String :YearBuilt
    String :TotalRooms
    String :Bedrooms
    String :HalfBathrooms
    String :FullBathrooms
    String :BasementGarage
    String :Basement
    String :FinishedBasement
    String :BasementType
    String :ExternalWalls
    String :NumberOfStories
    String :SquareFootageFinishedLiving
  end

  db.create_table! :parcel_boundary_area do
    Integer :GPIN, primary_key: true
    Integer :OBJECTID # also unique
    String  :geometry
  end
end

# collect all json records to find fields and types of fields
# yield "type fieldname" strings suitable for cut'n'pasting into schema definitions above
# NOTE assumes that types are at most things like [Integer, NilClass] and discards NilClass
def field_types_of json_filename
  return enum_for __method__, json_filename unless block_given?
    collected_types = records_of(json_filename).reduce(Hash.new) do |memo,record|
      types = record.transform_values(&:class)
      memo.merge(types){|_key,oldv,newv| [*Array(oldv), *Array(newv)].uniq}
    end

    collected_types.each do |k,v|
      field_type = case v.reject{|t| t == NilClass}
        in [t] then t
      else
          puts "don't know what type to use for #{v}, so using #{v.first}"
          v.first
      end
      yield "#{field_type} #{k.inspect}"
    end
  end

# shortcuts for tables for use with Sequel
# connect the sqlite db. create sqlite file if it doesn't already exist
def sqlite_db
  @sqlite_db ||= begin
    db = Sequel.connect 'sqlite://fire-risk.sqlite'
    # in case we want to tweak the schema and then dump it
    db.extension :schema_dumper
    db
  end
end

def pg_db; @pdb ||= Sequel.connect "postgres://fire-risk@localhost/fire-risk"; end

def db; pg_db end

def mad; @mad = db[:master_address_table] end
def pba; @pba = db[:parcel_boundary_area] end
def pad; @pad = db[:parcel_area_details] end
def recd; @recd = db[:real_estate_commercial_details] end
def rerd; @rerd = db[:real_estate_residential_details] end

# fetch the records from one of the geojson files
# and squash lower level geometry key with keys in properties.
#
# IE
#
# {
#   "type": "FeatureCollection",
#   "features": [
#     {
#       "type": "Feature",
#       "properties": {
#         "GPIN": 3,
#         "OBJECTID": 1
#       },
#       "geometry": {
#         "type": "Polygon",
#         "coordinates": [
# … }
#
# to
#
# [
#   {
#     "properties": {
#       "GPIN": 3,
#       "OBJECTID": 1
#       "geometry": {
#         "type": "Polygon",
#         "coordinates": [
# … ]
#
def records_of json_filename
  return enum_for __method__, json_filename unless block_given?

  # read the .json file
  json = JSON.parse File.read json_filename

  # features is always the top-level key, so just get its contents
  # and iterate over those
  json['features'].each do |item|
    # ruby-specific - we want keys as symbols
    # TODO might need to lowercase field names, or something like that.
    record_hash = item['properties'].transform_keys(&:intern)

    # add geometry field to json object keys, if necessary
    if (geometry = item['geometry'])
      record_hash[:geometry] = JSON.dump geometry
    end

    yield record_hash
  end
end

# import to db from the json file name
def import_table db, name
  table = db[name.intern]
  # transaction to speed up inserts
  db.transaction do
    # fetch all json "records" and insert them
    records_of "#{name}.json" do |hash| table.insert hash end
  end
end

def import db
  sources.each{|source| import_table db, source }
end

# Import all the .json files, provided they've been downloaded already
# Requires the db to exist already.
# If it doesn't, in bash say
#
#   createdb -U fire-risk fire-risk
#
def populate_db
  db = Sequel.connect 'postgres://localhost/fire-risk'
  schema db
  import db
end

# Find fields with the same name but different data
# to execute say
#
#   Array Overlaps.all_overlap_fields
#
module Overlaps
  module_function def overlaps
    sources.each_with_object Hash.new{|h,k| h[k] = []} do |source, field_tables|
      source = source.intern
      db[source].columns.each do |col|
        field_tables[col] << source
      end
    end
  end

  # These fields are common across tables, but either they're join fields, or we
  # know their contents will be legitimately (ie not accidentally) different
  # across tables.
  module_function def non_join_overlaps
    overlaps.
      select{|k,v| v.count > 1}.
      reject{|k,_v| case k when :OBJECTID, :geometry, :ParcelNumber then true end}
  end

  # the tables share this field, so create #{field}_#{table} and get their values
  # in the join
  module_function def overlap_fields field, tables
    fst, *rst = tables
    ds = rst.reduce db[fst] do |stmt,tbl|
      stmt.join(tbl, :ParcelNumber => :ParcelNumber)
    end

    fields = tables.map{|tbl| Sequel[tbl][field].as( [tbl, field].join('_') )}
    ds.select(Sequel[fst][:ParcelNumber], *fields)
  end

  module_function def check_overlap_fields field, tables
    return enum_for __method__, field, tables unless block_given?

    overlap_fields(field, tables).each do |ha|
      # only yield if the field values are not all the same value
      yield ha unless ha.reject{|k,v| k == :ParcelNumber}.values.uniq.size == 1
    end
  end

  module_function def all_overlap_fields
    return enum_for __method__ unless block_given?
    non_join_overlaps.each do |field, tables|
      yield [field, tables, Array(check_overlap_fields(field, tables))]
    end
  end
end
